{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example code of how LLama tokenizes text\n",
    "\n",
    "Llama 3 family uses a version of BPE similar enough to OpenAI's tiktoken. So we can just implement a toy version of it.\n",
    "(See https://github.com/meta-llama/llama3/blob/main/llama/tokenizer.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import tiktoken\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13225, 11, 1495, 553, 481, 30]\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "\n",
    "# Tokenize a text\n",
    "text = \"Hello, how are you?\"\n",
    "tokens = tokenizer.encode(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello'\n",
      "b','\n",
      "b' how'\n",
      "b' are'\n",
      "b' you'\n",
      "b'?'\n"
     ]
    }
   ],
   "source": [
    "#show which token string corresponds to each token id\n",
    "for token in tokens:\n",
    "    print(tokenizer.decode_single_token_bytes(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[155983, 1033, 54886, 151240, 35632, 4509, 11, 59965, 11, 138381, 11, 220, 5320, 11680, 11, 85805, 220, 41954, 13, 7633, 13, 19354, 12, 4388]\n"
     ]
    }
   ],
   "source": [
    "text = \"José da Silva Souza Monteiro, brasileiro, advogado, 43 anos, CPF 053.123.456-78\"\n",
    "tokens = tokenizer.encode(text)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show which token string corresponds to each token id\n",
    "token_bytes = [tokenizer.decode_single_token_bytes(token) for token in tokens]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'Jos\\xc3\\xa9',\n",
       " b' da',\n",
       " b' Silva',\n",
       " b' Souza',\n",
       " b' Monte',\n",
       " b'iro',\n",
       " b',',\n",
       " b' brasileiro',\n",
       " b',',\n",
       " b' advogado',\n",
       " b',',\n",
       " b' ',\n",
       " b'43',\n",
       " b' anos',\n",
       " b',',\n",
       " b' CPF',\n",
       " b' ',\n",
       " b'053',\n",
       " b'.',\n",
       " b'123',\n",
       " b'.',\n",
       " b'456',\n",
       " b'-',\n",
       " b'78']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['José',\n",
       " ' da',\n",
       " ' Silva',\n",
       " ' Souza',\n",
       " ' Monte',\n",
       " 'iro',\n",
       " ',',\n",
       " ' brasileiro',\n",
       " ',',\n",
       " ' advogado',\n",
       " ',',\n",
       " ' ',\n",
       " '43',\n",
       " ' anos',\n",
       " ',',\n",
       " ' CPF',\n",
       " ' ',\n",
       " '053',\n",
       " '.',\n",
       " '123',\n",
       " '.',\n",
       " '456',\n",
       " '-',\n",
       " '78']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show the tokens as strings, not bytes\n",
    "token_s_l = [token.decode('utf-8', errors='replace') for token in token_bytes]\n",
    "token_s_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simulate lllm-style embeddings for each token\n",
    "#create a random matrix of len(tokens) x llama-3-1-8b-embedding-dim\n",
    "import numpy as np\n",
    "llama_embedding_dim = 4096\n",
    "embedding_matrix = np.random.rand(len(tokens), llama_embedding_dim)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32698\n",
      "986\n",
      "35932\n",
      "1036\n",
      "6808\n",
      "63037\n",
      "1194\n",
      "430\n",
      "190647\n",
      "171448\n",
      "11\n",
      "59965\n",
      "138381\n",
      "220\n",
      "5320\n",
      "11680\n",
      "85805\n",
      "41954\n",
      "13\n",
      "7633\n",
      "19354\n",
      "12\n",
      "4388\n"
     ]
    }
   ],
   "source": [
    "#create a dictionary with the token as key and the embedding as value\n",
    "embedding_dict = {token: embedding for token, embedding in zip(tokens, embedding_matrix)}\n",
    "\n",
    "for key in embedding_dict.keys():\n",
    "    print(key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.71597359 0.12929455 0.43666185 ... 0.51584709 0.68081917 0.12534674]\n",
      " CPF\n"
     ]
    }
   ],
   "source": [
    "#show a random embedding from the dictionary and the string corresponding to the token\n",
    "import random\n",
    "random_key = random.choice(list(embedding_dict.keys()))\n",
    "print(embedding_dict[random_key])\n",
    "print(tokenizer.decode_single_token_bytes(random_key).decode('utf-8', errors='replace'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meta_anpd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
